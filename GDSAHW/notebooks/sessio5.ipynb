La tasca d'aquesta setmana consisteix en dur a terme les llistes de distàncies obtingudes a partir de l'extracció de característiques mitjançant Bag of Words.

Recapitulant de l'entrega anterior, s'han dut a terme numerosos progressos. Finalment s'ha pogut compilar tot el codi de suport i s'ha obtingut i instal·lat la versió 2.4.13 de la llibreria openCV2 a l'IDE pyCharm.

No obstant, s'han modificat seccions de codi referents a funcions que havien quedat obsoletes, com la obtenció de l'histograma de valors en un vector de strings. No suportat per la versió actual.

En primer terme, es defineixen les capçaleres:

from utils.params import get_params
import sys

from utils.rootsift import RootSIFT
import os, time
import numpy as np
import pickle
import cv2
from sklearn.cluster import MiniBatchKMeans
from sklearn.preprocessing import normalize, StandardScaler
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings("ignore")

# We need to add the source code path to the python path if we want to call modules such as 'utils'
params = get_params()
sys.path.insert(0,params['src'])


Tot i que el temps de computació és més llarg, s'obtenen precisions notablement més properes a 1.0 amb 1024 clústers. Igualment, reduïm en menor mesura (a 500 px) la mida de les imatges, a fi d'incrementar la precisió. Seguim utilitzant el descriptor i detector SIFT, amb la distància euclidiana com a mètrica.

A continuació obtenim els fitxers de característiques del conjunt d'entrenament i en determinem el temps emprat.

 # snippet from params.py :
 
 # Parameters
    params['split'] = 'val'
    params['descriptor_size'] = 1024 # Number of clusters
    params['descriptor_type'] = 'SIFT'
    params['keypoint_type'] = 'SIFT'
    params['max_size'] = 500 # Width size
    params['distance_type'] = 'euclidean'
    params['save_for_kaggle'] = True
 
 # Normalization of local descriptors
    params['whiten'] = True
    params['normalize_feats'] = True
    params['scale'] = False

Tal com es proposa utilitzem MiniBatchKMeans degut a la encara més elevada llista de característiques.

Operating on training set.
Stacking features...
('Done. Time elapsed:', 380.6548118591309)
('Feature count: ', (509065, 128))

Havent obtingut la llista de característiques, es computen les assignacions per construir el Bag of Words. Aqustes es normalitzen i a més hem indicat a params.py que ajusti els nivells amb l'ànim d'igualar l'histograma d'intensitats de píxel.

Training codebook...
('Done. Time elapsed:', 117.79228496551514)

A més a més, s'ha creat a la carpeta de sortida els diccionaris que han resultat del còmput de característiques.

Un cop desats els descriptors, procedim a ordenar-los segons la seva distància euclidiana al centroide del clúster. En definitiva, representa una mètrica de la semblança de la imatge d'entrada amb les del conjunt de validació.

Extracting BOW features...
('Done. Time elased:', 256.0032508702087)
Extracting BOW features...
('Done. Time elapsed:', 244.07635617256165)

A partir d'aquest punt, se'n calcula la precisió mitjana o MAP. L'indicador que mostra amb quina certesa la imatge d'entrada pot ésser reconeguda correctament.

Definitivement, el valor obtingut mostra com de lluny estava el ranking aleatori a l'hora d'ordenar els classificadors.

Igualment, com es proposa, obtenim la precisió mitjana en funció de cada família de fotografies; excepte les que no pertanyen a cap dels edificis anunciats.

0.338575478728
mercat_independencia 0.13887894098
societat_general 0.238183223065
farmacia_albinyana 0.363080853863
ajuntament 0.706307151243
mnactec 0.175387410694
escola_enginyeria 0.225500824943
masia_freixa 0.105686132788
castell_cartoixa 0.180288205398
dona_treballadora 0.0680786131359
catedral 0.174057750293
teatre_principal 0.392754080371
estacio_nord 0.0947025579631

Destaquen l'estació del Nord i el monument a la dona treballadora, amb uns resultats pobres.

Finalment, utilitzem l'script de mostra de les millors concordançes i si aquesta és correcta o incorrecta.